# plot average goal amount by category
category_goal_amounts <- explore_data %>%
group_by(mainCategory) %>%
summarize(avg_goal = mean(goal))
category_goal_plot <- ggplot(data = category_goal_amounts, aes(x = reorder(mainCategory, avg_goal), y = avg_goal)) +
geom_col(stat = 'identity') +
xlab('\n Campaign Category') +
ylab('Average Campgian Goal (USD)\n') +
ggtitle('Average Goal Amounts by Category') +
theme(plot.title = element_text(hjust = 0.5)) +
coord_flip()
category_goal_plot
# plot average amount pledged by category
category_pledge_amounts <- explore_data %>%
group_by(mainCategory) %>%
summarize(avg_pledge = mean(totalPledge))
category_pledge_plot <- ggplot(data = category_pledge_amounts, aes(x = reorder(mainCategory, avg_pledge), y = avg_pledge)) +
geom_col(stat = 'identity') +
xlab('\n Campaign Category') +
ylab('Average Amount Pledged (USD)\n') +
ggtitle('Average Pledge Amounts by Category') +
theme(plot.title = element_text(hjust = 0.5)) +
coord_flip()
category_pledge_plot
# average % overfunded for successful, average % underfunded for failed
avg_over <- mean(successfulCampaigns$percentageGoal)
avg_under <- mean(failedCampaigns$percentageGoal)
over_under <- tibble(
Campaign = c('Successful', 'Failed'),
PercentageGoal = c(avg_over, avg_under)
)
over_under
percent_success_plot <- ggplot(data = successfulCampaigns, aes(x = percentageGoal)) +
geom_histogram(bins = 25) +
xlab('\n % of Goal Pledged') +
ylab('Frequency\n') +
ggtitle('Distribution of % of Goal Pledged for Successful Campaigns') +
theme(plot.title = element_text(hjust = 0.5)) +
xlim(100,300)
percent_success_plot
summary(successfulCampaigns$percentageGoal)
percent_fail_plot <- ggplot(data = failedCampaigns, aes(x = percentageGoal)) +
geom_histogram(bins = 25) +
xlab('\n % of Goal Pledged') +
ylab('Frequency\n') +
ggtitle('Distribution of % of Goal Pledged for Failed Campaigns') +
theme(plot.title = element_text(hjust = 0.5))
percent_fail_plot
summary(failedCampaigns$percentageGoal)
#-------------------------- Binary Classification Models ----------------------------------------
# divided dataset randomly into training and test sets (80% training, 20% test)
#indicies <- sample(1:nrow(model_data), size=0.2*nrow(model_data))
#test <- model_data[indicies,]
#train <- model_data[-indicies,]
#write.csv(test, 'test.csv')
#write.csv(train, 'train.csv')
test <- read.csv('test.csv')
train <- read.csv('train.csv')
#-------------------------- Logistic Regression -------------------------------------------------
# set up for logistic regression model, most importantly specifying which features to omit
num_vars <- 19
measureVar <- "success"
features <- colnames(model_data, do.NULL = TRUE)
features_to_remove <- c("success", "campaignTitle", "description", "url", "creator",
"location", "valuta", "currency", "viewGallery", "countryState",
"city", "numLiveCampaigns", "category", "numSuccessfulCampaigns",
"comments")
features <- features[!features %in% features_to_remove]
modelSoFar <- paste(measureVar, " ~ 1 ")
possible <- features
# tibble to summarize models generated by forward stepwise regression
log_reg_summary <- tibble(
num_features = 0,
feature_added = "",
equation = "",
auc_training = 0,
auc_test = 0
)
# using forward stepwise regression to determine the best model
# for each number of possible features in the model
for(i in 1:num_vars) {
# initialize vectors to store AUC values for training and test sets
AUCs_train <- rep(0, length(possible))
AUCs_test <- rep(0, length(possible))
# for each remaining feature, add it to the current model
for(j in 1:length(possible)) {
formula <- paste(modelSoFar, " + ", possible[j])
sprintf(formula)
logreg_model <- glm(as.formula(formula), data = train, family = 'binomial')
# make predictions on the training set using the model
p <- predict(logreg_model, train, type = "response")
pred <- prediction(p, train$success)
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
AUCs_train[j] <- auc
# make predictions on the test set using the model
p <- predict(logreg_model, test, type = "response")
pred <- prediction(p, test$success)
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
AUCs_test[j] <- auc
# summarize model performance
log_reg_summary <- add_row(log_reg_summary, num_features = i, feature_added = possible[j],
equation = formula, auc_training = AUCs_train[j],
auc_test = AUCs_test[j])
}
# after comparing each remaining feature, choose the feature that improved the test AUC the most
max_index <- which.max(AUCs_test)
modelSoFar <- paste(modelSoFar, " + ", possible[max_index])
possible <- possible[-max_index]
}
log_reg_summary <- log_reg_summary[-1,]
# for each number of features, select the best model
best_models_by_num_features <- log_reg_summary %>%
group_by(num_features) %>%
arrange(desc(auc_test)) %>%
slice(1)
# plot of training and test AUCs for the best model for each number of features
auc_plot <- ggplot(data = best_models_by_num_features, aes(x = num_features)) +
geom_line(aes(y = auc_training, color = "training")) +
geom_line(aes(y = auc_test, color = "test")) +
xlab('\nNumber of Features') +
ylab('AUC\n') +
ggtitle('Training & Test AUC vs # of Features\n') +
theme(plot.title = element_text(hjust = 0.5)) +
scale_colour_manual("",
breaks = c("training", "test"),
values = c("red","blue"))
auc_plot
# select best model from the above list by choosing the one with the highest test AUC
threshold <- 0.5
max_index <- which.max(best_models_by_num_features$auc_test)
best_model <- glm(as.formula(best_models_by_num_features$equation[max_index]), data = train, family = 'binomial')
summary(best_model)
test<- test %>%
mutate(prob_success = predict(best_model, test, type = 'response'),
pred_success = (prob_success > threshold))
best_models_by_num_features$equation[max_index]
# compute accuracy of the best model
accuracy <- mean(test$pred_success == test$success)
# compute precision of the best model
pos_predictions <- test %>%
filter(pred_success == 1)
precision <- mean(pos_predictions$pred_success == pos_predictions$success)
# compute recall of the best model
pos_results <- test %>%
filter(success == 1)
recall <- mean(pos_results$pred_success == pos_results$success)
# summarize best model performance
best_model_summary <- tibble(
measure = c('Accuracy', 'Precision', 'Recall'),
value = c(accuracy, precision, recall)
)
best_model_summary
#--------------------------- Regularization for Logistic Regression -----------------------------
# L1 regularization model
x <- model.matrix(as.formula(best_models_by_num_features$equation[max_index]), train)[,-1]
y<- train$success
model_L1 <- glmnet(x, y, alpha = 1, lambda = 0.01, family="binomial")
# L2 regularization model
model_L2 <- glmnet(x, y, alpha = 0, lambda = 0.01, family="binomial")
# new predictions
x <- model.matrix(as.formula(best_models_by_num_features$equation[max_index]), test)[,-1]
p_L1 <- predict(model_L1, newx = x, type = "response")
p_L2 <- predict(model_L2, newx = x, type = "response")
pred_L1 <- prediction(p_L1, test$success)
auc_L1 <- performance(pred_L1, "auc")
auc_L1 <- unlist(slot(auc_L1, "y.values"))
pred_L2 <- prediction(p_L2, test$success)
auc_L2 <- performance(pred_L2, "auc")
auc_L2 <- unlist(slot(auc_L2, "y.values"))
summary_regularization <- tibble(
method = c("Original", "L1", "L2"),
value = c(best_models_by_num_features$auc_test[max_index], auc_L1, auc_L2)
)
summary_regularization
summary(best_model)
View(best_models_by_num_features)
summary(best_model)
# Included libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(ROCR)
library(glmnet)
library(SparseM)
library(e1071)
theme_set(theme_bw())
#------------------------------------ IMPORT DATA FILES ------------------------------------------
# Set working directory
setwd("~")
setwd('Documents/Miscellaneous/Kickstarter')
# full dataset (all features)
explore_data <- read_csv('explore.csv')
# dataset with edited features - removed any information that would not be known at the time of campaign launch
model_data <- read_csv('model.csv')
nlp_data <- read_csv('nlp.csv')
model_data <- inner_join(model_data, nlp_data)
# 4000 most backed Kickstarter campgians ever
most_backed <- read_csv('most_backed.csv')
#4000 live Kickstarter projects
live_projects <- read_csv('live.csv')
descriptions <- write.csv(model_data$description, 'descriptions.csv')
titles <- write.csv(model_data$campaignTitle, 'titles.csv')
#-------------------------- Exploratory Data Analysis -------------------------------------------
makeWordCloud <- function(data, photoTitle) {
corpus <- Corpus(VectorSource(data))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
colorPal <- brewer.pal(8,"Dark2")
png(photoTitle, width=2400,height=2400)
wordcloud(corpus, max.words = 60, random.order = FALSE, min.freq = 3, colors = colorPal)
dev.off()
}
# generate word cloud from the descriptions of the 4000 most backed kickstarter campaigns
mostBackedBlurbs <- read_file('mostBackedBlurbs.txt')
makeWordCloud(mostBackedBlurbs, "Most Backed Campaigns Wordcloud")
mostBackedTitles <- read_file('mostBackedTitles.txt')
makeWordCloud(mostBackedTitles, 'Most Backed Titles Wordcloud')
# successful campaigns included in the dataset that will be used to build the model
successfulCampaigns <- explore_data %>%
filter(success == 1)
# failed campaigns included in the dataset that will be used to build the model
failedCampaigns <- explore_data %>%
filter(success == 0)
# generate word clouds from the descriptions of the successful and failed campaigns
makeWordCloud(successfulCampaigns$description, "Successful Campaigns Wordcloud")
makeWordCloud(failedCampaigns$description, "Failed Campaigns Wordcloud")
# plot distribution of goal fundraising amounts for successful and failed campaigns
goal_success_plot <- ggplot(data = successfulCampaigns, aes(x = goal)) +
geom_histogram(bins = 50) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Goal Amounts for Successful Campaigns') +
theme(plot.title = element_text(hjust = 0.5))
goal_success_plot
goal_success_plot_zoomed <- ggplot(data = successfulCampaigns, aes(x = goal)) +
geom_histogram(bins = 25) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Goal Amounts for Successful Campaigns') +
theme(plot.title = element_text(hjust = 0.5)) +
xlim(0,50000)
goal_success_plot_zoomed
summary(successfulCampaigns$goal)
goal_fail_plot <- ggplot(data = failedCampaigns, aes(x = goal)) +
geom_histogram(bins = 50) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Goal Amounts for Failed Campaigns') +
theme(plot.title = element_text(hjust = 0.5))
goal_fail_plot
goal_fail_plot_zoomed <- ggplot(data = failedCampaigns, aes(x = goal)) +
geom_histogram(bins = 25) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Goal Amounts for Failed Campaigns') +
theme(plot.title = element_text(hjust = 0.5)) +
xlim(0,50000)
goal_fail_plot_zoomed
summary(failedCampaigns$goal)
# plot distribution of amounts raised for successful and failed campaigns
amount_success_plot <- ggplot(data = successfulCampaigns, aes(x = totalPledge)) +
geom_histogram(bins = 50) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Pledge Amounts for Successful Campaigns') +
theme(plot.title = element_text(hjust = 0.5))
amount_success_plot
amount_success_plot_zoomed <- ggplot(data = successfulCampaigns, aes(x = totalPledge)) +
geom_histogram(bins = 25) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Pledge Amounts for Successful Campaigns') +
theme(plot.title = element_text(hjust = 0.5)) +
xlim(0, 100000)
amount_success_plot_zoomed
# successful campaigns that raised more than $1M
successful_outliers <- successfulCampaigns %>%
filter(totalPledge > 1000000)
summary(successfulCampaigns$totalPledge)
amount_fail_plot <- ggplot(data = failedCampaigns, aes(x = totalPledge)) +
geom_histogram(bins = 50) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Pledge Amounts for Failed Campaigns') +
theme(plot.title = element_text(hjust = 0.5))
amount_fail_plot
amount_fail_plot_zoomed <- ggplot(data = failedCampaigns, aes(x = totalPledge)) +
geom_histogram(bins = 25) +
xlab('\n Goal Amount (USD)') +
ylab('Frequency\n') +
ggtitle('Distribution of Pledge Amounts for Failed Campaigns') +
theme(plot.title = element_text(hjust = 0.5)) +
xlim(0, 10000) +
ylim(0,200)
amount_fail_plot_zoomed
summary(failedCampaigns$totalPledge)
# failed campaigns that raised $40,000+
failed_outliers <- failedCampaigns %>%
filter(totalPledge > 40000)
# plot distribution of campaign duration for successful and failed campaigns
successful_duration_plot <- ggplot(data = successfulCampaigns, aes(x = duration)) +
geom_histogram(bins = 20) +
xlab('\n Duration (days)') +
ylab('Frequency\n') +
ggtitle('Distribution of Successful Campaign Durations') +
theme(plot.title = element_text(hjust = 0.5))
successful_duration_plot
summary(successfulCampaigns$duration)
failed_duration_plot <- ggplot(data = failedCampaigns, aes(x = duration)) +
geom_histogram(bins = 20) +
xlab('\n Duration (days)') +
ylab('Frequency\n') +
ggtitle('Distribution of Failed Campaign Durations') +
theme(plot.title = element_text(hjust = 0.5))
failed_duration_plot
summary(failedCampaigns$duration)
# plot success rates by category/subcategory
category_success_rates <- explore_data %>%
group_by(mainCategory) %>%
summarize(success_rate = 100* mean(success))
category_success_plot <- ggplot(data = category_success_rates, aes(x = reorder(mainCategory, success_rate), y = success_rate)) +
geom_col(stat = 'identity') +
xlab('\n Campaign Category') +
ylab('Success Rate (%)\n') +
ggtitle('Campaign Success Rates by Category') +
theme(plot.title = element_text(hjust = 0.5)) +
coord_flip()
category_success_plot
# plot average goal amount by category
category_goal_amounts <- explore_data %>%
group_by(mainCategory) %>%
summarize(avg_goal = mean(goal))
category_goal_plot <- ggplot(data = category_goal_amounts, aes(x = reorder(mainCategory, avg_goal), y = avg_goal)) +
geom_col(stat = 'identity') +
xlab('\n Campaign Category') +
ylab('Average Campgian Goal (USD)\n') +
ggtitle('Average Goal Amounts by Category') +
theme(plot.title = element_text(hjust = 0.5)) +
coord_flip()
category_goal_plot
# plot average amount pledged by category
category_pledge_amounts <- explore_data %>%
group_by(mainCategory) %>%
summarize(avg_pledge = mean(totalPledge))
category_pledge_plot <- ggplot(data = category_pledge_amounts, aes(x = reorder(mainCategory, avg_pledge), y = avg_pledge)) +
geom_col(stat = 'identity') +
xlab('\n Campaign Category') +
ylab('Average Amount Pledged (USD)\n') +
ggtitle('Average Pledge Amounts by Category') +
theme(plot.title = element_text(hjust = 0.5)) +
coord_flip()
category_pledge_plot
# average % overfunded for successful, average % underfunded for failed
avg_over <- mean(successfulCampaigns$percentageGoal)
avg_under <- mean(failedCampaigns$percentageGoal)
over_under <- tibble(
Campaign = c('Successful', 'Failed'),
PercentageGoal = c(avg_over, avg_under)
)
over_under
percent_success_plot <- ggplot(data = successfulCampaigns, aes(x = percentageGoal)) +
geom_histogram(bins = 25) +
xlab('\n % of Goal Pledged') +
ylab('Frequency\n') +
ggtitle('Distribution of % of Goal Pledged for Successful Campaigns') +
theme(plot.title = element_text(hjust = 0.5)) +
xlim(100,300)
percent_success_plot
summary(successfulCampaigns$percentageGoal)
percent_fail_plot <- ggplot(data = failedCampaigns, aes(x = percentageGoal)) +
geom_histogram(bins = 25) +
xlab('\n % of Goal Pledged') +
ylab('Frequency\n') +
ggtitle('Distribution of % of Goal Pledged for Failed Campaigns') +
theme(plot.title = element_text(hjust = 0.5))
percent_fail_plot
summary(failedCampaigns$percentageGoal)
#-------------------------- Binary Classification Models ----------------------------------------
# divided dataset randomly into training and test sets (80% training, 20% test)
#indicies <- sample(1:nrow(model_data), size=0.2*nrow(model_data))
#test <- model_data[indicies,]
#train <- model_data[-indicies,]
#write.csv(test, 'test.csv')
#write.csv(train, 'train.csv')
test <- read.csv('test.csv')
train <- read.csv('train.csv')
#-------------------------- Logistic Regression -------------------------------------------------
# set up for logistic regression model, most importantly specifying which features to omit
num_vars <- 19
measureVar <- "success"
features <- colnames(model_data, do.NULL = TRUE)
features_to_remove <- c("success", "campaignTitle", "description", "url", "creator",
"location", "valuta", "currency", "viewGallery", "countryState",
"city", "numLiveCampaigns", "category", "numSuccessfulCampaigns",
"comments")
features <- features[!features %in% features_to_remove]
modelSoFar <- paste(measureVar, " ~ 1 ")
possible <- features
# tibble to summarize models generated by forward stepwise regression
log_reg_summary <- tibble(
num_features = 0,
feature_added = "",
equation = "",
auc_training = 0,
auc_test = 0
)
# using forward stepwise regression to determine the best model
# for each number of possible features in the model
for(i in 1:num_vars) {
# initialize vectors to store AUC values for training and test sets
AUCs_train <- rep(0, length(possible))
AUCs_test <- rep(0, length(possible))
# for each remaining feature, add it to the current model
for(j in 1:length(possible)) {
formula <- paste(modelSoFar, " + ", possible[j])
sprintf(formula)
logreg_model <- glm(as.formula(formula), data = train, family = 'binomial')
# make predictions on the training set using the model
p <- predict(logreg_model, train, type = "response")
pred <- prediction(p, train$success)
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
AUCs_train[j] <- auc
# make predictions on the test set using the model
p <- predict(logreg_model, test, type = "response")
pred <- prediction(p, test$success)
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
AUCs_test[j] <- auc
# summarize model performance
log_reg_summary <- add_row(log_reg_summary, num_features = i, feature_added = possible[j],
equation = formula, auc_training = AUCs_train[j],
auc_test = AUCs_test[j])
}
# after comparing each remaining feature, choose the feature that improved the test AUC the most
max_index <- which.max(AUCs_test)
modelSoFar <- paste(modelSoFar, " + ", possible[max_index])
possible <- possible[-max_index]
}
log_reg_summary <- log_reg_summary[-1,]
# for each number of features, select the best model
best_models_by_num_features <- log_reg_summary %>%
group_by(num_features) %>%
arrange(desc(auc_test)) %>%
slice(1)
# plot of training and test AUCs for the best model for each number of features
auc_plot <- ggplot(data = best_models_by_num_features, aes(x = num_features)) +
geom_line(aes(y = auc_training, color = "training")) +
geom_line(aes(y = auc_test, color = "test")) +
xlab('\nNumber of Features') +
ylab('AUC\n') +
ggtitle('Training & Test AUC vs # of Features\n') +
theme(plot.title = element_text(hjust = 0.5)) +
scale_colour_manual("",
breaks = c("training", "test"),
values = c("red","blue"))
auc_plot
# select best model from the above list by choosing the one with the highest test AUC
threshold <- 0.5
max_index <- which.max(best_models_by_num_features$auc_test)
best_model <- glm(as.formula(best_models_by_num_features$equation[max_index]), data = train, family = 'binomial')
summary(best_model)
test<- test %>%
mutate(prob_success = predict(best_model, test, type = 'response'),
pred_success = (prob_success > threshold))
best_models_by_num_features$equation[max_index]
# compute accuracy of the best model
accuracy <- mean(test$pred_success == test$success)
# compute precision of the best model
pos_predictions <- test %>%
filter(pred_success == 1)
precision <- mean(pos_predictions$pred_success == pos_predictions$success)
# compute recall of the best model
pos_results <- test %>%
filter(success == 1)
recall <- mean(pos_results$pred_success == pos_results$success)
# summarize best model performance
best_model_summary <- tibble(
measure = c('Accuracy', 'Precision', 'Recall'),
value = c(accuracy, precision, recall)
)
best_model_summary
#--------------------------- Regularization for Logistic Regression -----------------------------
# L1 regularization model
x <- model.matrix(as.formula(best_models_by_num_features$equation[max_index]), train)[,-1]
y<- train$success
model_L1 <- glmnet(x, y, alpha = 1, lambda = 0.01, family="binomial")
# L2 regularization model
model_L2 <- glmnet(x, y, alpha = 0, lambda = 0.01, family="binomial")
# new predictions
x <- model.matrix(as.formula(best_models_by_num_features$equation[max_index]), test)[,-1]
p_L1 <- predict(model_L1, newx = x, type = "response")
p_L2 <- predict(model_L2, newx = x, type = "response")
pred_L1 <- prediction(p_L1, test$success)
auc_L1 <- performance(pred_L1, "auc")
auc_L1 <- unlist(slot(auc_L1, "y.values"))
pred_L2 <- prediction(p_L2, test$success)
auc_L2 <- performance(pred_L2, "auc")
auc_L2 <- unlist(slot(auc_L2, "y.values"))
summary_regularization <- tibble(
method = c("Original", "L1", "L2"),
value = c(best_models_by_num_features$auc_test[max_index], auc_L1, auc_L2)
)
summary_regularization
summary(best_model)
